---
title: "Variable Continuity Report"
author: "Carly Adams, MS"
date: "`r Sys.Date()`"
format:
  html:
    theme: lumen  # Try different themes: flatly, lumen, united, etc.
    self-contained: false  # Prevents missing graphs
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Load required libraries
library(survey)
library(srvyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(openxlsx)
library(knitr)

```

## Introduction

This analysis examines whether the survey question represented by V098 in 2019 and V276 in 2023 is consistent enough to be trended across survey years given the changes in response options.

**Question:**\
*"*Do you agree or disagree that in your community you feel like you matter to people?*."*

## **Changes in Response Option Order**

One potential source of inconsistency is the change in response option order between 2019 and 2023:

| Response Option | 2019 (V098)       | 2023 (V276)       |
|-----------------|-------------------|-------------------|
| A               | Strongly Agree    | Strongly Agree    |
| B               | Agree             | Agree             |
| C               | **Not Sure**      | Disagree          |
| D               | Disagree          | Strongly Disagree |
| E               | Strongly Disagree | **Not Sure**      |

In 2019, "Not Sure" was in a neutral opinion position, while in 2023, its placement may suggest a more deliberate uncertainty.

------------------------------------------------------------------------

## Data Import and Recoding

```{r import-data}

# Load raw data
raw_data <- read.xlsx("raw_yrbs_allyears_statewide.xlsx")

# Recode Responses for Consistency
filtered_data_raw <- raw_data %>%
  mutate(Response = case_when(
    Survey_Year == 2019 & V098 == 1 ~ "Strongly agree",
    Survey_Year == 2019 & V098 == 2 ~ "Agree",
    Survey_Year == 2019 & V098 == 3 ~ "Not Sure",
    Survey_Year == 2019 & V098 == 4 ~ "Disagree",
    Survey_Year == 2019 & V098 == 5 ~ "Strongly disagree",
    Survey_Year == 2023 & V276 == 1 ~ "Strongly agree",
    Survey_Year == 2023 & V276 == 2 ~ "Agree",
    Survey_Year == 2023 & V276 == 3 ~ "Disagree",
    Survey_Year == 2023 & V276 == 4 ~ "Strongly disagree",
    Survey_Year == 2023 & V276 == 5 ~ "Not Sure",
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(Response))
```

### **Interpretation:**

The response options for 2023 were reordered from 2019. This step ensures comparability by aligning them before analysis.

------------------------------------------------------------------------

## YRBS Complex Survey Design Setup

```{r survey-design}
# Define YRBS complex survey design
survey_design <- filtered_data_raw %>%
  as_survey_design(
    ids = Primary_Samp_Unit,
    strata = Stratum,
    weights = Final_Weight,
    nest = TRUE
  )
```

### **Interpretation:**

The dataset is now structured with survey weights, stratification, and PSU identifiers to maintain appropriate variance estimation.

------------------------------------------------------------------------

## Prevalence Estimates and Visualization

```{r prevalence-estimates}

# Calculate prevalence estimates
prevalence_results <- survey_design %>%
  group_by(Survey_Year, Response) %>%
  summarise(
    Prevalence = survey_mean(na.rm = TRUE,
                             proportion = TRUE,
                             prop_method = "logit", 
                             vartype = "ci"),
    Numerator = sum(Response == Response, na.rm = TRUE),
    Denominator = sum(!is.na(Response), na.rm = TRUE)
  ) %>%
  rename(
    `CI Low` = Prevalence_low,
    `CI High` = Prevalence_upp
  ) %>%
  mutate(
    Prevalence = round(Prevalence * 100, 1),
    `CI Low` = round(`CI Low` * 100, 1),
    `CI High` = round(`CI High` * 100, 1)
  )

```

```{r plot-prevalence, fig.height=5, fig.width=7}

ggplot(prevalence_results, aes(x = factor(Response, 
                                          levels = c(
  "Not Sure",
  "Strongly disagree",
  "Disagree",
  "Agree",
  "Strongly agree")),  
y = Prevalence, 
fill = factor(Survey_Year, levels = c(2019, 2023)))) +  
  geom_bar(stat = "identity", position =
             position_dodge(width = 0.9)) +  
  geom_errorbar(aes(ymin = `CI Low`, ymax = `CI High`), 
                width = 0.2, position = 
                  position_dodge(width = 0.9)) +  
  scale_fill_manual(values = c("2019" = "#F8766D", 
                               "2023" = "#00BFC4")) +  
  labs(title = "Feel They Matter in the Community - 
       Response Distributions (2019 vs 2023)",
       x = "Response Options",
       y = "Prevalence Estimate (%)",
       fill = "Survey Year") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, 
                                  size = 14, 
                                  face = "bold")) +
  coord_flip()
```

### **Interpretation:**

The bar chart displays prevalence estimates for each response category in 2019 and 2023, accounting for survey weights. As you can see there was a significant decrease in "Not Sure" and a significant increase in "Agree". When forced to make a decision, instead of staying neutral, more students seem to have chosen the more positive "Agree" option.

------------------------------------------------------------------------

## Rao-Scott Chi-Square Test

The **Rao-Scott Chi-Square test** was selected for statistical significance testing in this analysis because it accounts for the **complex survey design** of the YRBS data. The **Centers for Disease Control and Prevention (CDC) also uses this method** when analyzing YRBS data, as it properly adjusts for **stratification, clustering, and survey weights**—all critical factors in obtaining valid results.

The Rao-Scott test **corrects for the design effects** present in the YRBS, ensuring accurate significance testing even when **sample sizes vary between survey years**. Given these considerations, this test provides the most appropriate approach for evaluating variable continuity across survey cycles.

```{r chi-square-test}
chi_test <- svychisq(~ Response + Survey_Year, survey_design)
chi_test
```

### **Interpretation:**

**Test Statistic (F) = 16.528**

-   This is the adjusted chi-square statistic that tells us **how strong the difference is** between response distributions in 2019 and 2023.

-   A **higher F-value** indicates a **stronger difference**.

-   In general, an F-statistic above **4 or 5** suggests a meaningful difference—**16.528 is quite large**, meaning the response distributions **differ significantly**.

**Numerator Degrees of Freedom (ndf) = 3.795**

-   This reflects the **number of independent comparisons** being made while adjusting for the survey design.

-   Because of weighting and clustering, the effective degrees of freedom is slightly fractional rather than a whole number.

**Denominator Degrees of Freedom (ddf) = 455.393**

-   This represents the **available information for statistical inference**, adjusted for the survey design.

-   A **higher ddf (e.g., \>100)** means that the test has **a strong basis for detecting real differences** rather than random noise.

**p-value = 3.66e-12 (or \< 0.00000000000366)**

-   This is **extremely small**, meaning the probability of observing these differences **by random chance alone is virtually zero**.

-   A typical threshold for significance is **0.05**—our result is **far below this**, confirming that the differences are **statistically significant**.

This suggests that the observed changes are **not due to random sampling variation** but are likely influenced by **question wording, response order, or shifting interpretations**.

## Logistic Regression Results

The **logistic regression model** was used to check if responses in **2023 (V276) were significantly different from those in 2019 (V098)**. The model predicts response patterns based on the survey year while accounting for the **survey design (weights, clustering, stratification)**.

```{r ordinal-logit}

# Convert Response to an ordered factor
survey_design <- survey_design %>%
  mutate(Response = factor(Response, levels = c(
    "Not Sure", 
    "Strongly disagree",
    "Disagree",
    "Agree",
    "Strongly agree"), ordered = TRUE))


logit_model <- svyglm(as.numeric(Response) ~ factor(Survey_Year), 
                      design = survey_design, 
                      family = quasipoisson(link = "log"))

summary(logit_model)
```

### **Interpretation:**

**Estimate (0.1507)** – This tells us the **direction and size of the difference** between 2019 and 2023 responses.

-   A **positive value** means that, on average, responses were slightly **higher in 2023 than in 2019** (i.e., students were more likely to choose a response further down the scale). Higher Protective outcome.

-   If the estimate were **negative**, it would indicate lower response values in 2023 compared to 2019. Higher Risk outcome.

**Standard Error (0.0229)** – This measures **how precise** the estimate is.

-   The **standard error (SE)** tells us **how much variation** there is in our estimate due to sampling. It shows how precise our results are.

-   Since this is **a relatively small value**, it means that **our estimate is precise** and unlikely to vary much if we repeated the survey.

-   If the SE were **greater than 0.1 or 0.2**, it would indicate a **much higher level of uncertainty**, making the estimate less reliable.

**p-value (1.4e-09 or 0.0000000014)**

-   This tells us **if the difference between 2019 and 2023 is real or just due to random chance**.

-   A **very small p-value (less than 0.05)** means that the difference is **statistically significant**—it is **unlikely to have happened by chance**.

-   Here, **1.4e-09 is extremely small**, meaning that the difference is **highly significant**.

**What This Means for Our Data**

-   Responses **changed significantly** between 2019 and 2023.

-   The shift is **not due to random variation**—it is a **real difference**.

-   This confirms that **V098 (2019) and V276 (2023) should not be treated as the same variable for trending**.

By using this model, we ensure that **our decision to create a new V-code for 2023 was correct** and that we are not misrepresenting trends in the data.

------------------------------------------------------------------------

## Subgroup Analysis

Chi-square tests were conducted separately within demographic subgroups (Sex, Grade, Race_Group_3) to determine whether differences persisted across populations.

```{r}
# Look at Subgroups Chi Square test

subgroups <- c("Sex", "Grade", "Race_Group_3")
subgroup_results <- list()

for (subgroup in subgroups) {
  filtered_data <- filtered_data_raw %>% 
    filter(!is.na(!!sym(subgroup)))  
  subgroup_survey_design <- filtered_data %>%
    as_survey_design(
      ids = Primary_Samp_Unit,
      strata = Stratum,
      weights = Final_Weight,
      nest = TRUE
    )
  chi_test <- svychisq(
    as.formula(paste("~ Response + Survey_Year")), subgroup_survey_design)
  subgroup_results[[subgroup]] <- chi_test
}

# Print results
subgroup_results
```

### **Interpretation:**

| Subgroup     | F-statistic | df    | p-value  |
|--------------|-------------|-------|----------|
| Sex          | 16.638      | 3.822 | 2.60e-12 |
| Grade        | 16.245      | 3.795 | 5.79e-12 |
| Race_Group_3 | 16.824      | 3.779 | 2.50e-12 |

-   In **every subgroup**, the differences remain **highly significant**.

-   This suggests that the issue is **not isolated to a particular demographic group** but reflects **a broader inconsistency in the response structure.**

## Conclusion

Based on these findings, **V098 (2019) and V276 (2023) should not be trended together**. The **response order change, statistical significance in distributional shifts, and persistence of these differences across subgroups** indicate that a **new V-code decision was the correct course of action**.
